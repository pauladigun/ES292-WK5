---
title: "ES292"
---
output:
  pdf_document:
    latex_engine: xelatex
---

# 5E1.

Which of the linear models below are multiple linear regressions?

(2) μi = α + βxi + βzzi

(4) μi = β0 + β1xi + β2zi

Both of these models are multiple linear regression models because they include more than one predictor variable (xi and zi).

# 5E2.

Write down a multiple regression to evaluate the claim: Animal diversity is linearly related to latitude, but only after controlling for plant diversity. You just need to write down the model definition.

animal_diversityi ~ Normal(μi, σ)

μi = α + β1latitudei + β2plant_diversityi


Where:

Animal_diversity_i is the animal diversity in location i

latitude_i is the latitude of location i

plant_diversity_i is the plant diversity in location i

α is the intercept

β1 is the regression coefficient for latitude

β2 is the regression coefficient for plant diversity

σ is the error standard deviation

# 5E3. 

Write down a multiple regression to evaluate the claim: Neither amount of funding nor size of laboratory is by itself a good predictor of time to PhD degree; but together these variables are both positively associated with time to degree. Write down the model definition and indicate which side of zero each slope parameter should be on.

time_to_PhDi ~ Normal(μi, σ)  

μi = α + β1fundingi + β2lab_sizei

We expect both β1 and β2 to be positive
β1 > 0 
β2 > 0


Where:

time_to_PhDi is the time (in years) for individual i to complete their PhD

α is the intercept

β1 is the regression coefficient for fundingi (amount of funding for individual i)

β2 is the regression coefficient for lab_sizei (size of lab for individual i)




# 5E4.

Suppose you have a single categorical predictor with 4 levels (unique values), labeled A, B, C and D. Let Ai be an indicator variable that is 1 where case i is in category A. Also suppose Bi, Ci, and Di for the other categories. Now which of the following linear models are inferentially equivalent ways to include the categorical variable in a regression? Models are inferentially equivalent when it’s possible to compute one posterior distribution from the posterior distribution of another model.

1 μi = α + βA*Ai + βB*Bi + βD*Di

3 μi = α + βB*Bi + βC*Ci + βD*Di

4 μi = αA*Ai + αB*Bi + αC*Ci + αD*Di

5 μi = αA*(1 - Bi - Ci - Di) + αB*Bi + αC*Ci + αD*Di

The first regression model has an intercept term representing category C and regression coefficients for predictors A, B, and D. 

The second model cannot be identified because it tries to estimate a regression coefficient for every possible category.

The third model has an intercept term for category A and regression coefficients for predictors B, C, and D.

The fourth model takes a unique intercept approach with a separate intercept term estimated for each category and no regression coefficients.

The fifth model uses a reparameterization technique where the intercept for category A is multiplied by 1 when the observation is in category A and 0 otherwise. 

The first, third, fourth, and fifth models are inferentially equivalent in that the posterior distribution for the parameters in each model can be derived from the others. For example, the intercept and difference between categories can be computed from each model.

Except the 2nd model, others are all inferentially equivalent. The 2nd model should not include four indicators, since intercept has already existed in the formula.




# 5M1.


Invent your own example of a spurious correlation. An outcome variable should be correlated with both predictor variables. But when both predictors are entered in the same model, the correlation between the outcome and one of the predictors should mostly vanish (or at least be greatly reduced).




We can generate a simulated dataset by directly creating a data frame where each variable is sampled from a normal distribution. we will draw 100 values for each of 3 variables.

The first variable pred_1 represents our main predictor. 

The outcome Y will be related only to pred_1. 

The second predictor pred_2 will also depend on pred_1, 

The outcome variable is only related to the first predictor, but the second predictor is as well dependent on the first predictor

 To simplify choosing priors later, we will z-score standardize each variable by applying the scale() function, which centers to mean 0 and scales to standard deviation 1. 

```{r}
library(tidyverse)
N <- 100
dfr <- tibble(pred_1 = rnorm(N), 
       pred_2 = rnorm(N, -pred_1), 
       out_var = rnorm(N, pred_1)) %>% 
  mutate(across(everything(), scale))

pairs(dfr)

```



Now let’s see how the outcome is related to the first predictor within a linear regression using quadratic approximation: Notice that the used priors that are not flat but instead are within a realistic realm. 
α must be pretty close to 0 when we standardise the outcome and the predictor. The prior on the slope 
β is a bit more wider but still only captures realistic relationships as seen in prior predictive simulations in chapter 5

```{r}
library(rethinking)

m1 <- alist(out_var ~ dnorm(mu, sigma),
      mu <- a + B1*pred_1,
      a ~ dnorm(0, 0.2), 
      B1 ~ dnorm(0, 0.5),
      sigma ~ dexp(1)) %>% 
  quap(., data = dfr) %>% 
  precis() %>% 
  as_tibble(rownames = "estimate")


```



We will follow the same process for the second predictor variable and the outcome, utilizing similar prior distributions. Unlike the first predictor, this second predictor does not have a causal relationship with the outcome variable. However, these two variables still demonstrate a correlation

```{r}
m2 <- alist(out_var ~ dnorm(mu, sigma),
            mu <- a + B2*pred_2,
            a ~ dnorm(0, 0.2), 
            B2 ~ dnorm(0, 0.5),
            sigma ~ dexp(1)) %>% 
  quap(., data = dfr) %>% 
  precis() %>% 
  as_tibble(rownames = "estimate")

```




And finally we estimate a multiple linear regression including both predictor pred_1 and predictor pred_2 regressing on outcome Y. Since by construction only pred_1 causally impacts outcome in our simulated data, fitting this full model should reveal that true relationship while showing that adding pred_2 provides little additional explanatory ability, even though pred_2 is correlated with both pred_1 and Y outcome.

```{r}

m3 <- alist(out_var ~ dnorm(mu, sigma),
            mu <- a + B1*pred_1 + B2*pred_2,
            a ~ dnorm(0, 0.2),
            B1 ~ dnorm(0, 0.5),
            B2 ~ dnorm(0, 0.5),
            sigma ~ dexp(1)) %>% 
  quap(., data = dfr) %>% 
  precis() %>% 
  as_tibble(rownames = "estimate")

```





Now we can add the β estimates of each model, which capture the relationship between each predictor and the outcome, to a dataframe and plot it.

```{r}
library(ggplot2)

full_join(m1, m2) %>% 
  full_join(m3) %>% 
  add_column(model = rep(paste("Model", 1:3), c(3, 3, 4))) %>% 
  filter(estimate %in% c("B1", "B2")) %>% 
  mutate(combined = str_c(model, estimate, sep = ": ")) %>% 
  rename(lower_pi = '5.5%', upper_pi = '94.5%') %>% 
  ggplot() +
  geom_vline(xintercept = 0, colour = "grey20", alpha = 0.5, 
             linetype = "dashed") +
  geom_pointrange(aes(x = mean, xmin = lower_pi, xmax = upper_pi,  
                      combined, colour = estimate), size = 0.9,
                  show.legend = FALSE) +
  scale_color_manual(values = c("red", "blue")) +
  labs(y = NULL, x = "Estimate") +
  theme_classic()

```




The separate pred_2 only regression incorrectly identifies an association between pred_2 and the outcome Y. However, the multiple regression with both pred_1 and pred_2 demonstrates that pred_2 has no explanatory relationship with outcome Y beyond what is already captured by pred_1. This showcases how in the presence of correlated predictors, the multiple regression framework can delineate the unique, causal contributions and reveal that the bivariate pred2-outcome regression result is spurious rather than reflective of the true data generating process relating only pred1 to Y outcome. The multiple regression gets the right answer - pred_2 provides no new information for predicting Y once we account for pred_1.
```{r}
library(ggdag)

tibble(name = c("Outcome", "Predictor1", "Predictor2"),
       x    = c(1, 0, 2),
       y    = c(0, 1, 1)) %>% 
  dagify(Outcome ~ Predictor1,
         Predictor2 ~ Predictor1,
         coords = .) %>% 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_node(color = "red", alpha = 0.5) +
  geom_dag_text(aes(label = abbreviate(name)), color = "blue") +
  geom_dag_edges(edge_color = "blue") +
  theme_void()

```

# 5M2

We generate simulated data as in 5M1: sample predictor 1 normally, let predictor 2's means depend on predictor 1 to induce correlation, simulate outcome with positive correlation to predictor 1 and negative to predictor 2.


```{r}
N <- 100
dfr <- tibble(pred_1 = rnorm(N, sd = 3), 
              pred_2 = rnorm(N, pred_1, sd = 0.5), 
              out_var = rnorm(N, pred_1 - pred_2)) %>% 
  mutate(across(everything(), scale))


```

We fit two bivariate regressions on each predictor singly and one multiple regression on both predictors, then tidy the resulting model estimates for comparison.


```{r}

# bivariate regression of predictor 1 on the outcome
m1 <- alist(out_var ~ dnorm(mu, sigma),
            mu <- a + B1*pred_1,
            a ~ dnorm(0, 0.2), 
            B1 ~ dnorm(0, 0.5),
            sigma ~ dexp(1)) %>% 
  quap(., data = dfr) %>% 
  precis() %>% 
  as_tibble(rownames = "estimate")



# bivariate regression of predictor 2 on the outcome
m2 <- alist(out_var ~ dnorm(mu, sigma),
            mu <- a + B2*pred_2,
            a ~ dnorm(0, 0.2), 
            B2 ~ dnorm(0, 0.5),
            sigma ~ dexp(1)) %>% 
  quap(., data = dfr) %>% 
  precis() %>% 
  as_tibble(rownames = "estimate")



# multiple linear regression of predictor 1 and predictor 2 on the outcome
m3 <- alist(out_var ~ dnorm(mu, sigma),
            mu <- a + B1*pred_1 + B2*pred_2,
            a ~ dnorm(0, 0.2),
            B1 ~ dnorm(0, 0.5),
            B2 ~ dnorm(0, 0.5),
            sigma ~ dexp(1)) %>% 
  quap(., data = dfr) %>% 
  precis() %>% 
  as_tibble(rownames = "estimate")

```


We can combine all model estimates into one data frame, process it for visualization then plot to compare regression results.


```{r}
full_join(m1, m2) %>% 
  full_join(m3) %>% 
  add_column(model = rep(paste("Model", 1:3), c(3, 3, 4))) %>% 
  filter(estimate %in% c("B1", "B2")) %>% 
  mutate(combined = str_c(model, estimate, sep = ": ")) %>% 
  rename(lower_pi = '5.5%', upper_pi = '94.5%') %>% 
  ggplot() +
  geom_pointrange(aes(x = mean, xmin = lower_pi, xmax = upper_pi,  
                      combined, colour = estimate), size = 1, 
                  show.legend = FALSE) +
  geom_vline(xintercept = 0, colour = "grey20", 
             linetype = "dashed", alpha = 0.5) +
  scale_color_manual(values = c("red", "blue")) +
  labs(y = NULL, x = "Estimate") +
  theme_classic()


```


The plots showcase how the true relationships between the outcome and predictors were obscured in the separate pred_1-only and pred_2-only bivariate regressions. However, the full multiple regression reveals the accurate positive association with pred_1 and absence of pred_2 effect. Since pred_1 and pred_2 are correlated, there is likely an unobserved common cause inducing this dependency between the two predictors.


```{r}

tibble(name = c("Outcome", "Pred1", "Pred2", "Unobserved"),
       x    = c(1, 0, 2, 1),
       y    = c(0, 1, 1, 1.5)) %>% 
  dagify(Outcome ~ Pred1 + Pred2,
         Pred1 ~ Unobserved,
         Pred2 ~ Unobserved,
         coords = .) %>% 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_node(color = "red", alpha = 0.5) +
  geom_dag_text(color = "blue") +
  geom_dag_edges(edge_color = "blue") +
  theme_void()



```


# 5H1
# Hard. 

All three use same data, Since urban foxes move in packs and defend territories, data on habitat quality and population density is also included.The data frame has five columns:
(1) group: Number of the social group the individual fox belongs to (2) avgfood: The average amount of food available in the territory (3) groupsize: The number of foxes in the social group
(4) area: Size of the territory
(5) weight: Body weight of the individual fox


# 5H1. 

Fit two bivariate Gaussian regressions, using quap:


Model weight as a bivariate Gaussian distribution conditioned on area.
Model weight as a bivariate Gaussian distribution conditioned on group size.



```{r}
data(foxes)
d <- foxes
summary(d)

```

(1) body weight as a linear function of territory size (area)

```{r}

d$area.s <- (d$area - mean(d$area)) / sd(d$area)
mod5 <- map(
  alist(
    weight ~ dnorm( mu, sigma) ,
    mu <- a + bA*area.s ,
    a ~ dnorm( 0, 100),
    bA ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ), data=d
)
plot( precis( mod5))
```

It appears that area may not significantly predict body weight. 


(2) body weight as a linear function of groupsize. Plot the results of these regressions, displaying the MAP regression line and the 95% interval of the mean. Is either variable important for predicting fox body weight?

```{r}
area.seq <- seq(from=-2.5, to=2.5, length.out = 300)
mu <- link(mod5, data=list(area.s = area.seq))
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob=0.95)
weight.sim <- sim(mod5, data=list(area.s = area.seq))
weight.HPDI <- apply( weight.sim, 2, HPDI, prob=0.95)
plot(  weight ~ area.s, data=d)
lines( area.seq, mu.mean)
shade( mu.HPDI, area.seq)
shade( weight.HPDI, area.seq)

```

This plot further indicates that area may not be a significant predictor. It appears there is little to no discernible relationship with area, and the variability (sigma) is relatively high.

```{r}
d$groupsize.s <- (d$groupsize - mean(d$groupsize)) / sd(d$groupsize)
mod6 <- map(
  alist(
    weight ~ dnorm( mu, sigma) ,
    mu <- a + bG*groupsize.s,
    bG ~ dnorm(0, 1),
    a ~ dnorm(0, 10),
    sigma ~ dunif(0, 10)
  ), data=d
)
plot( precis( mod6))
```

The group size appears to hold some degree of significance, as its parameter estimate deviates slightly further from zero compared to the parameter for area.

```{r}
groupsize.seq <- seq(from=-2, to=3, length.out = 300)
mu <- link( mod6, data=list(groupsize.s=groupsize.seq))
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob=0.95)
weight.sim <- sim(mod6, data=list(groupsize.s=groupsize.seq))
weight.HPDI <- apply(weight.sim, 2, HPDI, prob=0.95)
plot( weight ~ groupsize.s, data=d)
lines(groupsize.seq, mu.mean)
shade(mu.HPDI, groupsize.seq)
shade(weight.HPDI, groupsize.seq)
```



# 5H2.

Now fit a multiple linear regression with weight as the out come and both area and group size as predictor variables. Plot the predictions of the model for each predictor, holding the other predictor constant at its mean. What does this model say about the importance of each variable? Why do you get different results than you got in the exercise just above?


```{r}
mod7 <- map(
  alist(
    weight ~ dnorm(mu, sigma),
    mu <- a + bA*area.s + bG*groupsize.s,
    a ~ dnorm(0, 10),
    bA ~ dnorm(0,1),
    bG ~ dnorm(0,1),
    sigma ~ dunif(0,10)
  ), data=d
)
plot( precis( mod7))

```



Both area and group size appear to hold noticeable significance in predicting body weight. Let's generate plots displaying the model predictions for both predictors, while keeping the other predictor constant at its mean value.



Area, holding groupsize constant

```{r}


groupsize.avg <- mean(d$groupsize.s)
area.seq <- seq(from=-3, to=3, length.out = 300)
pred.data <- data.frame(groupsize.s=groupsize.avg, area.s=area.seq)
mu <- link(mod7, data=pred.data)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob=0.95)
weight.sim <- sim(mod7, data=pred.data)
weight.HPDI <- apply(weight.sim, 2, HPDI, prob=0.95)
plot( weight ~ area.s, data=d, type="n" )
lines(area.seq, mu.mean)
shade(mu.HPDI, area.seq)
shade(weight.HPDI, area.seq)

```



We observe a distinct association between area and weight: Foxes inhabiting larger territories appear to have greater weight compared to those in smaller territories.


Groupsize,  holding area constant


```{r}


area.avg <- mean(d$area.s)
groupsize.seq <- seq(from=-2, to=3, length.out = 300)
pred.data <- data.frame(groupsize.s=groupsize.seq, area.s=area.avg)
mu <- link(mod7, data=pred.data)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob=0.95)
weight.sim <- sim(mod7, data=pred.data)
weight.HPDI <- apply(weight.sim, 2, HPDI, prob=0.95)
plot( weight ~ groupsize.s, data=d, type="n")
lines( groupsize.seq, mu.mean)
shade( mu.HPDI, groupsize.seq)
shade( weight.HPDI, groupsize.seq)

```


A larger group is associated with a lower weight

```{r}
pairs( weight ~ groupsize + area, data=d)

```

Group size demonstrates a strong correlation with area: larger territories are linked with larger fox groups (which intuitively aligns). Furthermore, both area and group size show correlations with weight: area positively correlates with weight, whereas group size negatively correlates with weight. This situation results in a concealed association: the effects of the two predictor variables counterbalance each other, obscuring their individual importance.

# 5H3.

Finally, consider the avgfood variable. Fit two more multiple regressions: (1) body weight as an additive function of avgfood and groupsize, and (2) body weight as an additive function of all three variables, avgfood and groupsize and area. Compare the results of these models to the previous models you’ve fit, in the first two exercises. 
(b) When both avgfood or area are in the same model, their effects are reduced (closer to zero) and their standard errors are larger than when they are included in separate models. Can you explain this result?


Let's introduce the average amount of food variable into our analysis. We will then conduct two additional multivariate regression analyses.

We'll explore two more multivariate regression models:


1) weight ~ avgfood + groupsize


```{r}
d$avgfood.s <- (d$avgfood - mean(d$avgfood)) / sd(d$avgfood)
mod8 <- map(
  alist(
    weight ~ dnorm( mu, sigma),
    mu <- a + bF*avgfood.s + bG*groupsize.s,
    a ~ dnorm(0, 10),
    bF ~ dnorm(0,1),
    bG ~ dnorm(0,1),
    sigma ~ dunif(0,10)
  ), data=d
)
plot( precis( mod8))

```


2

weight ~ avgfood + groupsize + area

```{r}
mod9 <- map(
  alist(
    weight ~ dnorm( mu, sigma),
    mu <- a + bF*avgfood.s + bG*groupsize.s + bA*area.s,
    a ~ dnorm(0, 10),
    c(bF, bG, bA) ~ dnorm(0, 1),
    sigma ~ dunif(0, 10)
  ), data=d
)
plot( precis( mod9))

```


Including both the average amount of food and the area of the territory results in a reduction in the parameter estimates for both predictor variables, compared to the regressions containing only one of the two. 


(a) Is avgfood or area a better predictor of body weight? If you had to choose one or the other to include in a model, which would it be? Support your assessment with any tables or plots you choose.

lets show the residual plot 
```{r}
mod9.1 <- map(
  alist(
    avgfood.s ~ dnorm( mu, sigma) ,
    mu <- a + b*area.s,
    a ~ dnorm(0, 10),
    b ~ dnorm(0,1),
    sigma ~ dunif(0, 10)
  ), data=d
)

# cpmpute residuals
mu <- coef(mod9.1)['a'] + coef(mod9.1)['b']*d$area.s

# compute residuals
f.resid <- d$avgfood.s - mu

plot( avgfood.s ~ area.s, d, col=rangi2)
abline(mod9.1)

for (i in 1:length(f.resid)) {
  x <- d$area.s[i]
  y <- d$avgfood.s[i]
  # draw the lines segment
  lines( c(x,x), c(mu[i], y), lwd=0.5, col=col.alpha("black", 0.5))
}

```



The residual plot illustrates the average amount of food remaining after accounting for the linear relationship with area. Each line segment represents a residual, indicating the observed average amount of food's distance from the expected value when attempting to predict it solely with area. Foxes positioned above the regression line exhibit more food than anticipated, while those below possess less, relative to expectations based on area alone. These residuals represent the variance in average amount of food remaining after removing the purely linear association between area and average food. Utilizing these residuals, we can plot them against the actual outcome of interest—weight. Such plots are commonly referred to as predictor residual plots.


```{r}

plot( d$weight ~ f.resid, col=rangi2, xlab="Average food residuals", ylab="Weight")
abline(a=coef(mod9)['a'], b=coef(mod9)['bF'])
abline(v=0, lty=2)
```

```{r}

# Predictor residual plot
mod9.2 <- map(
  alist(
    area.s ~ dnorm( mu, sigma) ,
    mu <- a + b*avgfood.s,
    a ~ dnorm(0, 10),
    b ~ dnorm(0,1),
    sigma ~ dunif(0, 10)
  ), data=d
)

# cpmpute residuals
mu <- coef(mod9.2)['a'] + coef(mod9.2)['b']*d$avgfood.s

# compute residuals
a.resid <- d$area.s - mu

plot( d$weight ~  a.resid, col=rangi2, xlab="Area residuals", ylab="Weight")
abline(a=coef(mod9)['a'], b=coef(mod9)['bA'])
abline(v=0, lty=2)


for (i in 1:length(f.resid)) {
  x <- d$area.s[i]
  y <- d$avgfood.s[i]
  # draw the lines segment
  lines( c(x,x), c(mu[i], y), lwd=0.5, col=col.alpha("black", 0.5))
}


```


The vertical dashed line denotes an area that precisely aligns with the expectation derived from the average amount of food. Consequently, points positioned to the right of the line denote foxes possessing more area than anticipated given their average food, while those to the left indicate less area than expected for their average food amount. On both sides of the line, we observe approximately similar weights.

```{r}
# counterfactual plot of average food, holding area and groupsize fixed
area.avg <- mean(d$area.s)
groupsize.avg <- mean(d$groupsize.s)
avgfood.seq <- seq(from=-2, to=3)
data.pred <- data.frame(area.s=area.avg,
                        groupsize.s=groupsize.avg,
                        avgfood.s=avgfood.seq)
mu <- link(mod9, data=data.pred)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI)
weight.sim <- sim(mod9, data=data.pred)
weight.HPDI <- apply(weight.sim, 2, HPDI, prob=0.95)
plot( weight ~ avgfood.s, data=d, type="n")
lines( avgfood.seq, mu.mean)
shade( mu.HPDI, avgfood.seq)
shade( weight.HPDI, avgfood.seq)

```

Now the same for area:
```{r}

# counterfactual plot for area, holding groupsize and average food fixed
avgfood.avg <- mean(d$avgfood.s)
groupsize.avg <- mean(d$groupsize.s)
area.seq <- seq(from=-3, to=3)
data.pred <- data.frame(area.s=area.seq,
                        groupsize.s=groupsize.avg,
                        avgfood.s=avgfood.avg)
mu <- link(mod9, data=data.pred)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI)
weight.sim <- sim(mod9, data=data.pred)
weight.HPDI <- apply(weight.sim, 2, HPDI, prob=0.95)
plot( weight ~ area.s, data=d, type="n")
lines(area.seq, mu.mean)
shade(mu.HPDI, area.seq)
shade(weight.HPDI, area.seq)


```

The two predictor variables display very similar relationships with the outcome, indicating they provide largely redundant information in predicting the outcome. Based on the similarity of the predictor-outcome plots, there is little basis from these visualizations alone to justify selecting one variable over the other for inclusion in the model. I would advise against incorporating both predictors simultaneously due to their high intercorrelation, as evidenced in the following visualization:


```{r}
pairs(weight ~ avgfood + groupsize + area, data=d)
```


```{r}
cor( d[,c("weight", "avgfood", "groupsize", "area")])

```

There are strong positive correlations (r > 0.8) between the predictor variables of average food, group size, and area. The exceptionally high correlations between average food and the other two predictors (approaching r = 0.9) indicate these relationships: Larger territories sustain more abundant food sources, which in turn support larger group sizes. However, such high intercorrelations imply redundancy among the predictors that creates issues of multicollinearity. This is evidenced by the substantially inflated standard errors and unstable coefficient estimates when both average food and area are included as predictors. Given area initially limits food availability, which consequently constrains group sizes that can be supported, area is likely the primary causal driver of ecological carrying capacity. I would therefore include only area in the predictive model since, as the original causal factor, it is least redundant with the other correlates. Nonetheless, reasonable arguments could also be made for average food or group size as appropriate predictors. As the textbook highlights, showing similar results across any of these strongly intercorrelated variables is a defensible modeling approach.